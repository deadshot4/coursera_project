{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "**************************************************\n",
      "test.tsv\n",
      "File data/test_embeddings.tsv is already downloaded.\n",
      "Downloading GoogleNews-vectors-negative300.bin.gz (1.5G) for you, it will take a while...\n",
      "***********************************************\n",
      "GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(fname='GoogleNews-vectors-negative300.bin',\n",
    "                                                  binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'Is', 'Your', 'Name', '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_test = 'What Is Your Name ?'\n",
    "question_test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    tokens = question.split()\n",
    "    word_embeddings = []\n",
    "    \n",
    "    # In case of words with no embeddings, then they are skipped\n",
    "    for word in tokens:\n",
    "        if word in embeddings:\n",
    "            word_embeddings.append(embeddings[word])\n",
    "    \n",
    "    # If the question doesn't contain any known word with embedding\n",
    "    if not word_embeddings:\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "    question_embeddings = np.mean(word_embeddings, axis=0)\n",
    "    \n",
    "    return question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import array_to_string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.01929389126598835\n",
      "-0.02872721292078495\n",
      "0.0460561104118824\n",
      "0.0852593332529068\n",
      "0.0243055559694767\n",
      "-0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    hit_counts = np.array(dup_ranks) <= k\n",
    "    \n",
    "    return np.mean(hit_counts, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    dup_ranks = np.array(dup_ranks)\n",
    "    dcg = (1/(np.log2(1 + dup_ranks))) * (dup_ranks <= k)\n",
    "    return np.mean(dcg, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    qv = question_to_vec(question, embeddings, dim)[np.newaxis, :]\n",
    "    cvs = np.array([question_to_vec(candidate, embeddings, dim) for candidate in candidates])\n",
    "    sims = cosine_similarity(qv, cvs)[0]\n",
    "    idxs = np.argsort(sims)[::-1]\n",
    "    return [(i, candidates[i]) for i in idxs]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.207 | Hits@   1: 0.207\n",
      "DCG@   5: 0.275 | Hits@   5: 0.334\n",
      "DCG@  10: 0.291 | Hits@  10: 0.385\n",
      "DCG@ 100: 0.339 | Hits@ 100: 0.621\n",
      "DCG@ 500: 0.368 | Hits@ 500: 0.847\n",
      "DCG@1000: 0.384 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(word) for word in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.331 | Hits@   1: 0.331\n",
      "DCG@   5: 0.400 | Hits@   5: 0.462\n",
      "DCG@  10: 0.420 | Hits@  10: 0.522\n",
      "DCG@ 100: 0.452 | Hits@ 100: 0.685\n",
      "DCG@ 500: 0.474 | Hits@ 500: 0.857\n",
      "DCG@1000: 0.489 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/train.tsv', 'data/train_prepared.tsv')\n",
    "prepare_file('data/validation.tsv', 'data/validation_prepared.tsv')\n",
    "prepare_file('data/test.tsv', 'data/test_prepared.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93.6%  lr: 0.040340  loss: 0.044067  eta: 0h6m  tot: 0h1m25s  (18.7%)%  lr: 0.049980  loss: 0.261852  eta: 0h4m  tot: 0h0m0s  (0.2%)%  lr: 0.049870  loss: 0.215005  eta: 0h8m  tot: 0h0m1s  (0.3%)1.9%  lr: 0.049850  loss: 0.207619  eta: 0h8m  tot: 0h0m1s  (0.4%)2.9%  lr: 0.049740  loss: 0.174659  eta: 0h9m  tot: 0h0m3s  (0.6%)3.1%  lr: 0.049690  loss: 0.167565  eta: 0h9m  tot: 0h0m3s  (0.6%)3.3%  lr: 0.049680  loss: 0.163494  eta: 0h9m  tot: 0h0m3s  (0.7%)0h9m  tot: 0h0m4s  (0.7%)3.5%  lr: 0.049650  loss: 0.159329  eta: 0h9m  tot: 0h0m4s  (0.7%)4.3%  lr: 0.049500  loss: 0.144820  eta: 0h9m  tot: 0h0m5s  (0.9%)%  lr: 0.049339  loss: 0.125749  eta: 0h9m  tot: 0h0m6s  (1.1%)7.9%  lr: 0.049149  loss: 0.111405  eta: 0h8m  tot: 0h0m8s  (1.6%)8.4%  lr: 0.049089  loss: 0.108221  eta: 0h8m  tot: 0h0m8s  (1.7%)8.6%  lr: 0.049049  loss: 0.107037  eta: 0h8m  tot: 0h0m9s  (1.7%)9.1%  lr: 0.048989  loss: 0.104958  eta: 0h8m  tot: 0h0m9s  (1.8%)9.5%  lr: 0.048949  loss: 0.103045  eta: 0h8m  tot: 0h0m9s  (1.9%)9.8%  lr: 0.048909  loss: 0.101496  eta: 0h8m  tot: 0h0m10s  (2.0%)10.3%  lr: 0.048809  loss: 0.099635  eta: 0h8m  tot: 0h0m10s  (2.1%)10.4%  lr: 0.048809  loss: 0.099046  eta: 0h8m  tot: 0h0m10s  (2.1%)10.5%  lr: 0.048799  loss: 0.098839  eta: 0h8m  tot: 0h0m10s  (2.1%)11.9%  lr: 0.048699  loss: 0.095949  eta: 0h8m  tot: 0h0m12s  (2.4%)12.1%  lr: 0.048679  loss: 0.095565  eta: 0h8m  tot: 0h0m12s  (2.4%)12.2%  lr: 0.048679  loss: 0.094962  eta: 0h8m  tot: 0h0m12s  (2.4%)13.2%  lr: 0.048609  loss: 0.092217  eta: 0h8m  tot: 0h0m13s  (2.6%)16.8%  lr: 0.048298  loss: 0.083381  eta: 0h8m  tot: 0h0m16s  (3.4%)17.5%  lr: 0.048238  loss: 0.082207  eta: 0h8m  tot: 0h0m17s  (3.5%)17.6%  lr: 0.048198  loss: 0.081786  eta: 0h8m  tot: 0h0m17s  (3.5%)19.4%  lr: 0.047978  loss: 0.078771  eta: 0h8m  tot: 0h0m19s  (3.9%)19.6%  lr: 0.047958  loss: 0.078434  eta: 0h8m  tot: 0h0m19s  (3.9%)21.2%  lr: 0.047858  loss: 0.076169  eta: 0h7m  tot: 0h0m21s  (4.2%)%  lr: 0.047838  loss: 0.076157  eta: 0h7m  tot: 0h0m21s  (4.2%)21.5%  lr: 0.047798  loss: 0.075768  eta: 0h7m  tot: 0h0m21s  (4.3%)22.3%  lr: 0.047688  loss: 0.074744  eta: 0h7m  tot: 0h0m22s  (4.5%)22.9%  lr: 0.047628  loss: 0.074380  eta: 0h7m  tot: 0h0m22s  (4.6%)23.0%  lr: 0.047608  loss: 0.074225  eta: 0h7m  tot: 0h0m23s  (4.6%)23.4%  lr: 0.047568  loss: 0.073657  eta: 0h7m  tot: 0h0m23s  (4.7%)23.7%  lr: 0.047538  loss: 0.073243  eta: 0h7m  tot: 0h0m23s  (4.7%)24.8%  lr: 0.047447  loss: 0.072002  eta: 0h7m  tot: 0h0m24s  (5.0%)25.0%  lr: 0.047437  loss: 0.071675  eta: 0h7m  tot: 0h0m24s  (5.0%)25.4%  lr: 0.047407  loss: 0.071143  eta: 0h7m  tot: 0h0m25s  (5.1%)26.1%  lr: 0.047347  loss: 0.070367  eta: 0h7m  tot: 0h0m25s  (5.2%)26.4%  lr: 0.047307  loss: 0.070066  eta: 0h7m  tot: 0h0m25s  (5.3%)26.7%  lr: 0.047277  loss: 0.069592  eta: 0h7m  tot: 0h0m26s  (5.3%)29.0%  lr: 0.047197  loss: 0.067653  eta: 0h7m  tot: 0h0m28s  (5.8%)31.3%  lr: 0.046927  loss: 0.065974  eta: 0h7m  tot: 0h0m30s  (6.3%)%  lr: 0.046887  loss: 0.065720  eta: 0h7m  tot: 0h0m31s  (6.3%)%  lr: 0.046867  loss: 0.065442  eta: 0h7m  tot: 0h0m31s  (6.4%)32.7%  lr: 0.046797  loss: 0.064832  eta: 0h7m  tot: 0h0m32s  (6.5%)33.3%  lr: 0.046747  loss: 0.064318  eta: 0h7m  tot: 0h0m32s  (6.7%)33.8%  lr: 0.046717  loss: 0.063959  eta: 0h7m  tot: 0h0m32s  (6.8%)34.9%  lr: 0.046547  loss: 0.063213  eta: 0h7m  tot: 0h0m33s  (7.0%)35.3%  lr: 0.046527  loss: 0.063017  eta: 0h7m  tot: 0h0m34s  (7.1%)36.6%  lr: 0.046396  loss: 0.062138  eta: 0h7m  tot: 0h0m35s  (7.3%)37.1%  lr: 0.046386  loss: 0.061766  eta: 0h7m  tot: 0h0m35s  (7.4%)37.6%  lr: 0.046336  loss: 0.061418  eta: 0h7m  tot: 0h0m36s  (7.5%)  eta: 0h7m  tot: 0h0m36s  (7.6%)39.1%  lr: 0.046156  loss: 0.060662  eta: 0h7m  tot: 0h0m37s  (7.8%)39.7%  lr: 0.046126  loss: 0.060280  eta: 0h7m  tot: 0h0m37s  (7.9%)39.8%  lr: 0.046116  loss: 0.060258  eta: 0h7m  tot: 0h0m38s  (8.0%)40.7%  lr: 0.046036  loss: 0.059711  eta: 0h7m  tot: 0h0m39s  (8.1%)41.8%  lr: 0.045856  loss: 0.059065  eta: 0h7m  tot: 0h0m40s  (8.4%)42.9%  lr: 0.045676  loss: 0.058551  eta: 0h7m  tot: 0h0m41s  (8.6%)43.7%  lr: 0.045616  loss: 0.058132  eta: 0h7m  tot: 0h0m41s  (8.7%)45.6%  lr: 0.045405  loss: 0.057225  eta: 0h7m  tot: 0h0m44s  (9.1%)45.7%  lr: 0.045375  loss: 0.057223  eta: 0h7m  tot: 0h0m44s  (9.1%)46.4%  lr: 0.045275  loss: 0.056962  eta: 0h7m  tot: 0h0m45s  (9.3%)%  lr: 0.044995  loss: 0.055947  eta: 0h7m  tot: 0h0m46s  (9.7%)49.5%  lr: 0.044915  loss: 0.055502  eta: 0h7m  tot: 0h0m47s  (9.9%)  tot: 0h0m48s  (10.1%)51.5%  lr: 0.044675  loss: 0.054743  eta: 0h7m  tot: 0h0m49s  (10.3%)52.7%  lr: 0.044575  loss: 0.054373  eta: 0h7m  tot: 0h0m50s  (10.5%)53.4%  lr: 0.044505  loss: 0.054134  eta: 0h7m  tot: 0h0m51s  (10.7%)54.4%  lr: 0.044374  loss: 0.053717  eta: 0h7m  tot: 0h0m52s  (10.9%)55.0%  lr: 0.044324  loss: 0.053577  eta: 0h7m  tot: 0h0m52s  (11.0%)55.4%  lr: 0.044304  loss: 0.053533  eta: 0h7m  tot: 0h0m52s  (11.1%)55.6%  lr: 0.044244  loss: 0.053442  eta: 0h7m  tot: 0h0m53s  (11.1%)55.7%  lr: 0.044224  loss: 0.053402  eta: 0h7m  tot: 0h0m53s  (11.1%)56.3%  lr: 0.044144  loss: 0.053196  eta: 0h7m  tot: 0h0m53s  (11.3%)56.4%  lr: 0.044114  loss: 0.053201  eta: 0h7m  tot: 0h0m53s  (11.3%)57.4%  lr: 0.044054  loss: 0.052913  eta: 0h7m  tot: 0h0m54s  (11.5%)58.0%  lr: 0.043984  loss: 0.052695  eta: 0h7m  tot: 0h0m55s  (11.6%)60.4%  lr: 0.043804  loss: 0.051915  eta: 0h6m  tot: 0h0m57s  (12.1%)60.6%  lr: 0.043794  loss: 0.051839  eta: 0h6m  tot: 0h0m57s  (12.1%)60.7%  lr: 0.043774  loss: 0.051806  eta: 0h6m  tot: 0h0m57s  (12.1%)61.8%  lr: 0.043624  loss: 0.051449  eta: 0h6m  tot: 0h0m58s  (12.4%)62.1%  lr: 0.043584  loss: 0.051362  eta: 0h6m  tot: 0h0m58s  (12.4%)63.1%  lr: 0.043414  loss: 0.050996  eta: 0h6m  tot: 0h0m59s  (12.6%)63.2%  lr: 0.043393  loss: 0.050966  eta: 0h6m  tot: 0h0m59s  (12.6%)%  lr: 0.043333  loss: 0.050723  eta: 0h6m  tot: 0h1m0s  (12.8%)64.3%  lr: 0.043313  loss: 0.050661  eta: 0h6m  tot: 0h1m1s  (12.9%)65.8%  lr: 0.043183  loss: 0.050154  eta: 0h6m  tot: 0h1m2s  (13.2%)66.4%  lr: 0.043123  loss: 0.049963  eta: 0h6m  tot: 0h1m2s  (13.3%)67.0%  lr: 0.043063  loss: 0.049775  eta: 0h6m  tot: 0h1m3s  (13.4%)68.9%  lr: 0.042893  loss: 0.049183  eta: 0h6m  tot: 0h1m5s  (13.8%)69.7%  lr: 0.042763  loss: 0.048967  eta: 0h6m  tot: 0h1m5s  (13.9%)70.7%  lr: 0.042643  loss: 0.048799  eta: 0h6m  tot: 0h1m6s  (14.1%)70.9%  lr: 0.042603  loss: 0.048711  eta: 0h6m  tot: 0h1m7s  (14.2%)71.5%  lr: 0.042573  loss: 0.048588  eta: 0h6m  tot: 0h1m7s  (14.3%)71.7%  lr: 0.042533  loss: 0.048524  eta: 0h6m  tot: 0h1m7s  (14.3%)71.9%  lr: 0.042533  loss: 0.048487  eta: 0h6m  tot: 0h1m8s  (14.4%)72.0%  lr: 0.042533  loss: 0.048480  eta: 0h6m  tot: 0h1m8s  (14.4%)73.7%  lr: 0.042342  loss: 0.048017  eta: 0h6m  tot: 0h1m9s  (14.7%)74.2%  lr: 0.042272  loss: 0.047898  eta: 0h6m  tot: 0h1m9s  (14.8%)74.4%  lr: 0.042252  loss: 0.047840  eta: 0h6m  tot: 0h1m10s  (14.9%)75.3%  lr: 0.042192  loss: 0.047632  eta: 0h6m  tot: 0h1m10s  (15.1%)75.6%  lr: 0.042162  loss: 0.047554  eta: 0h6m  tot: 0h1m11s  (15.1%)77.7%  lr: 0.042032  loss: 0.047036  eta: 0h6m  tot: 0h1m12s  (15.5%)79.9%  lr: 0.041842  loss: 0.046609  eta: 0h6m  tot: 0h1m14s  (16.0%)80.4%  lr: 0.041792  loss: 0.046539  eta: 0h6m  tot: 0h1m14s  (16.1%)80.5%  lr: 0.041792  loss: 0.046527  eta: 0h6m  tot: 0h1m14s  (16.1%)81.0%  lr: 0.041762  loss: 0.046443  eta: 0h6m  tot: 0h1m15s  (16.2%)81.3%  lr: 0.041722  loss: 0.046374  eta: 0h6m  tot: 0h1m15s  (16.3%)81.9%  lr: 0.041632  loss: 0.046226  eta: 0h6m  tot: 0h1m15s  (16.4%)82.6%  lr: 0.041552  loss: 0.046103  eta: 0h6m  tot: 0h1m16s  (16.5%)85.3%  lr: 0.041241  loss: 0.045511  eta: 0h6m  tot: 0h1m18s  (17.1%)%  lr: 0.041201  loss: 0.045449  eta: 0h6m  tot: 0h1m19s  (17.1%)86.5%  lr: 0.040991  loss: 0.045276  eta: 0h6m  tot: 0h1m20s  (17.3%)88.0%  lr: 0.040901  loss: 0.044979  eta: 0h6m  tot: 0h1m21s  (17.6%)88.6%  lr: 0.040871  loss: 0.044873  eta: 0h6m  tot: 0h1m21s  (17.7%)89.6%  lr: 0.040731  loss: 0.044691  eta: 0h6m  tot: 0h1m22s  (17.9%)92.2%  lr: 0.040471  loss: 0.044304  eta: 0h6m  tot: 0h1m24s  (18.4%)92.6%  lr: 0.040461  loss: 0.044208  eta: 0h6m  tot: 0h1m24s  (18.5%)0.040340  loss: 0.044042  eta: 0h6m  tot: 0h1m25s  (18.7%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.040040  loss: 0.042936  eta: 0h5m  tot: 0h1m29s  (20.0%)4.2%  lr: 0.040300  loss: 0.043966  eta: 0h6m  tot: 0h1m26s  (18.8%)94.3%  lr: 0.040290  loss: 0.043926  eta: 0h6m  tot: 0h1m26s  (18.9%)%  lr: 0.040250  loss: 0.043886  eta: 0h6m  tot: 0h1m26s  (18.9%)\n",
      " ---+++                Epoch    0 Train error : 0.04384994 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030040  loss: 0.012868  eta: 0h3m  tot: 0h2m46s  (40.0%)0%  lr: 0.039770  loss: 0.011504  eta: 0h6m  tot: 0h1m32s  (20.4%)2.3%  lr: 0.039730  loss: 0.012364  eta: 0h6m  tot: 0h1m32s  (20.5%)m  tot: 0h1m33s  (20.6%)4.3%  lr: 0.039550  loss: 0.012368  eta: 0h6m  tot: 0h1m34s  (20.9%)4.8%  lr: 0.039510  loss: 0.012203  eta: 0h6m  tot: 0h1m34s  (21.0%)5.9%  lr: 0.039439  loss: 0.011795  eta: 0h5m  tot: 0h1m35s  (21.2%)6.7%  lr: 0.039319  loss: 0.011722  eta: 0h5m  tot: 0h1m36s  (21.3%)6.8%  lr: 0.039299  loss: 0.011798  eta: 0h5m  tot: 0h1m36s  (21.4%)7.0%  lr: 0.039289  loss: 0.011804  eta: 0h5m  tot: 0h1m36s  (21.4%)8.6%  lr: 0.039149  loss: 0.012345  eta: 0h6m  tot: 0h1m38s  (21.7%)%  lr: 0.039089  loss: 0.012332  eta: 0h6m  tot: 0h1m38s  (21.8%)10.7%  lr: 0.038989  loss: 0.012312  eta: 0h5m  tot: 0h1m40s  (22.1%)11.0%  lr: 0.038959  loss: 0.012469  eta: 0h5m  tot: 0h1m40s  (22.2%)12.2%  lr: 0.038859  loss: 0.012621  eta: 0h5m  tot: 0h1m41s  (22.4%)12.2%  lr: 0.038849  loss: 0.012704  eta: 0h5m  tot: 0h1m41s  (22.4%)13.1%  lr: 0.038799  loss: 0.012610  eta: 0h5m  tot: 0h1m41s  (22.6%)13.3%  lr: 0.038789  loss: 0.012590  eta: 0h5m  tot: 0h1m41s  (22.7%)13.8%  lr: 0.038749  loss: 0.012508  eta: 0h5m  tot: 0h1m42s  (22.8%)14.0%  lr: 0.038729  loss: 0.012535  eta: 0h5m  tot: 0h1m42s  (22.8%)14.3%  lr: 0.038709  loss: 0.012639  eta: 0h5m  tot: 0h1m42s  (22.9%)14.4%  lr: 0.038709  loss: 0.012717  eta: 0h5m  tot: 0h1m42s  (22.9%)16.2%  lr: 0.038549  loss: 0.013170  eta: 0h5m  tot: 0h1m44s  (23.2%)18.1%  lr: 0.038358  loss: 0.013216  eta: 0h5m  tot: 0h1m45s  (23.6%)18.2%  lr: 0.038358  loss: 0.013230  eta: 0h5m  tot: 0h1m45s  (23.6%)18.5%  lr: 0.038318  loss: 0.013178  eta: 0h5m  tot: 0h1m46s  (23.7%)19.2%  lr: 0.038238  loss: 0.013007  eta: 0h5m  tot: 0h1m46s  (23.8%)19.4%  lr: 0.038218  loss: 0.012973  eta: 0h5m  tot: 0h1m46s  (23.9%)19.8%  lr: 0.038158  loss: 0.012949  eta: 0h5m  tot: 0h1m47s  (24.0%)19.9%  lr: 0.038148  loss: 0.012932  eta: 0h5m  tot: 0h1m47s  (24.0%)20.1%  lr: 0.038118  loss: 0.012938  eta: 0h5m  tot: 0h1m47s  (24.0%)20.6%  lr: 0.038068  loss: 0.012902  eta: 0h5m  tot: 0h1m47s  (24.1%)21.0%  lr: 0.038038  loss: 0.012793  eta: 0h5m  tot: 0h1m48s  (24.2%)21.5%  lr: 0.037998  loss: 0.012898  eta: 0h5m  tot: 0h1m48s  (24.3%)22.0%  lr: 0.037958  loss: 0.012927  eta: 0h5m  tot: 0h1m48s  (24.4%)22.5%  lr: 0.037868  loss: 0.012963  eta: 0h5m  tot: 0h1m49s  (24.5%)%  lr: 0.037838  loss: 0.012953  eta: 0h5m  tot: 0h1m49s  (24.6%)25.4%  lr: 0.037678  loss: 0.012788  eta: 0h5m  tot: 0h1m51s  (25.1%)27.0%  lr: 0.037478  loss: 0.012834  eta: 0h5m  tot: 0h1m53s  (25.4%)0.037468  loss: 0.012811  eta: 0h5m  tot: 0h1m53s  (25.5%)27.7%  lr: 0.037447  loss: 0.012804  eta: 0h5m  tot: 0h1m53s  (25.5%)28.4%  lr: 0.037317  loss: 0.012841  eta: 0h5m  tot: 0h1m54s  (25.7%)31.6%  lr: 0.036977  loss: 0.012964  eta: 0h5m  tot: 0h1m56s  (26.3%)32.0%  lr: 0.036947  loss: 0.012938  eta: 0h5m  tot: 0h1m57s  (26.4%)32.0%  lr: 0.036947  loss: 0.012939  eta: 0h5m  tot: 0h1m57s  (26.4%)33.8%  lr: 0.036727  loss: 0.012927  eta: 0h5m  tot: 0h1m58s  (26.8%)34.4%  lr: 0.036657  loss: 0.012951  eta: 0h5m  tot: 0h1m59s  (26.9%)34.6%  lr: 0.036647  loss: 0.012944  eta: 0h5m  tot: 0h1m59s  (26.9%)35.0%  lr: 0.036587  loss: 0.012998  eta: 0h5m  tot: 0h2m0s  (27.0%)35.6%  lr: 0.036547  loss: 0.013044  eta: 0h5m  tot: 0h2m0s  (27.1%)36.6%  lr: 0.036396  loss: 0.013041  eta: 0h5m  tot: 0h2m1s  (27.3%)37.6%  lr: 0.036256  loss: 0.013055  eta: 0h5m  tot: 0h2m2s  (27.5%)37.8%  lr: 0.036206  loss: 0.013048  eta: 0h5m  tot: 0h2m2s  (27.6%)38.6%  lr: 0.036096  loss: 0.013053  eta: 0h5m  tot: 0h2m3s  (27.7%)39.5%  lr: 0.035996  loss: 0.013095  eta: 0h5m  tot: 0h2m4s  (27.9%)0.035876  loss: 0.013057  eta: 0h5m  tot: 0h2m5s  (28.2%)41.1%  lr: 0.035806  loss: 0.013089  eta: 0h5m  tot: 0h2m6s  (28.2%)41.5%  lr: 0.035786  loss: 0.013149  eta: 0h5m  tot: 0h2m6s  (28.3%)41.6%  lr: 0.035756  loss: 0.013144  eta: 0h5m  tot: 0h2m6s  (28.3%)41.8%  lr: 0.035756  loss: 0.013139  eta: 0h5m  tot: 0h2m6s  (28.4%)43.6%  lr: 0.035556  loss: 0.013157  eta: 0h5m  tot: 0h2m7s  (28.7%)44.7%  lr: 0.035506  loss: 0.013116  eta: 0h5m  tot: 0h2m8s  (28.9%)44.9%  lr: 0.035425  loss: 0.013102  eta: 0h5m  tot: 0h2m8s  (29.0%)45.4%  lr: 0.035345  loss: 0.013093  eta: 0h5m  tot: 0h2m9s  (29.1%)46.8%  lr: 0.035225  loss: 0.013138  eta: 0h5m  tot: 0h2m10s  (29.4%)47.0%  lr: 0.035195  loss: 0.013125  eta: 0h5m  tot: 0h2m10s  (29.4%)47.1%  lr: 0.035185  loss: 0.013115  eta: 0h5m  tot: 0h2m11s  (29.4%)0.035005  loss: 0.013125  eta: 0h5m  tot: 0h2m12s  (29.7%)48.8%  lr: 0.034995  loss: 0.013123  eta: 0h5m  tot: 0h2m12s  (29.8%)0.034955  loss: 0.013120  eta: 0h5m  tot: 0h2m12s  (29.8%)49.4%  lr: 0.034885  loss: 0.013120  eta: 0h5m  tot: 0h2m13s  (29.9%)49.7%  lr: 0.034875  loss: 0.013130  eta: 0h5m  tot: 0h2m13s  (29.9%)50.2%  lr: 0.034845  loss: 0.013126  eta: 0h5m  tot: 0h2m14s  (30.0%)m  tot: 0h2m14s  (30.1%)54.0%  lr: 0.034545  loss: 0.013009  eta: 0h4m  tot: 0h2m16s  (30.8%)54.1%  lr: 0.034545  loss: 0.012995  eta: 0h4m  tot: 0h2m16s  (30.8%)56.9%  lr: 0.034214  loss: 0.013038  eta: 0h4m  tot: 0h2m18s  (31.4%)57.4%  lr: 0.034194  loss: 0.013006  eta: 0h4m  tot: 0h2m18s  (31.5%)57.5%  lr: 0.034184  loss: 0.013025  eta: 0h4m  tot: 0h2m18s  (31.5%)58.9%  lr: 0.034024  loss: 0.013006  eta: 0h4m  tot: 0h2m19s  (31.8%)59.0%  lr: 0.034024  loss: 0.012995  eta: 0h4m  tot: 0h2m19s  (31.8%)59.1%  lr: 0.034014  loss: 0.013001  eta: 0h4m  tot: 0h2m19s  (31.8%)59.9%  lr: 0.033894  loss: 0.012971  eta: 0h4m  tot: 0h2m19s  (32.0%)60.0%  lr: 0.033884  loss: 0.012971  eta: 0h4m  tot: 0h2m20s  (32.0%)62.6%  lr: 0.033644  loss: 0.012985  eta: 0h4m  tot: 0h2m21s  (32.5%)63.2%  lr: 0.033604  loss: 0.012965  eta: 0h4m  tot: 0h2m21s  (32.6%)64.4%  lr: 0.033514  loss: 0.012934  eta: 0h4m  tot: 0h2m22s  (32.9%)64.5%  lr: 0.033504  loss: 0.012946  eta: 0h4m  tot: 0h2m22s  (32.9%)65.5%  lr: 0.033363  loss: 0.012990  eta: 0h4m  tot: 0h2m22s  (33.1%)66.3%  lr: 0.033253  loss: 0.013017  eta: 0h4m  tot: 0h2m23s  (33.3%)0.032863  loss: 0.012938  eta: 0h4m  tot: 0h2m24s  (34.1%)81.2%  lr: 0.031872  loss: 0.012949  eta: 0h3m  tot: 0h2m30s  (36.2%)81.5%  lr: 0.031822  loss: 0.012957  eta: 0h3m  tot: 0h2m31s  (36.3%)81.8%  lr: 0.031772  loss: 0.012938  eta: 0h3m  tot: 0h2m31s  (36.4%)82.0%  lr: 0.031722  loss: 0.012933  eta: 0h3m  tot: 0h2m31s  (36.4%)83.3%  lr: 0.031582  loss: 0.012921  eta: 0h3m  tot: 0h2m32s  (36.7%)%  lr: 0.031532  loss: 0.012910  eta: 0h3m  tot: 0h2m32s  (36.7%)  eta: 0h3m  tot: 0h2m33s  (36.8%)84.2%  lr: 0.031512  loss: 0.012910  eta: 0h3m  tot: 0h2m33s  (36.8%)84.5%  lr: 0.031472  loss: 0.012920  eta: 0h3m  tot: 0h2m33s  (36.9%)84.9%  lr: 0.031462  loss: 0.012919  eta: 0h3m  tot: 0h2m33s  (37.0%)86.2%  lr: 0.031361  loss: 0.012920  eta: 0h3m  tot: 0h2m35s  (37.2%)86.5%  lr: 0.031341  loss: 0.012924  eta: 0h3m  tot: 0h2m35s  (37.3%)87.2%  lr: 0.031271  loss: 0.012902  eta: 0h3m  tot: 0h2m35s  (37.4%)88.5%  lr: 0.031111  loss: 0.012879  eta: 0h3m  tot: 0h2m37s  (37.7%)89.5%  lr: 0.031041  loss: 0.012877  eta: 0h3m  tot: 0h2m38s  (37.9%)89.8%  lr: 0.030991  loss: 0.012873  eta: 0h3m  tot: 0h2m38s  (38.0%)0.030931  loss: 0.012874  eta: 0h3m  tot: 0h2m38s  (38.0%)90.7%  lr: 0.030841  loss: 0.012882  eta: 0h3m  tot: 0h2m39s  (38.1%)90.8%  lr: 0.030821  loss: 0.012888  eta: 0h3m  tot: 0h2m39s  (38.2%)91.0%  lr: 0.030811  loss: 0.012901  eta: 0h3m  tot: 0h2m39s  (38.2%)91.1%  lr: 0.030811  loss: 0.012900  eta: 0h3m  tot: 0h2m39s  (38.2%)92.6%  lr: 0.030711  loss: 0.012923  eta: 0h3m  tot: 0h2m40s  (38.5%)94.5%  lr: 0.030551  loss: 0.012883  eta: 0h3m  tot: 0h2m42s  (38.9%)94.9%  lr: 0.030491  loss: 0.012889  eta: 0h3m  tot: 0h2m42s  (39.0%)95.2%  lr: 0.030471  loss: 0.012891  eta: 0h3m  tot: 0h2m42s  (39.0%)97.2%  lr: 0.030290  loss: 0.012874  eta: 0h3m  tot: 0h2m43s  (39.4%)97.7%  lr: 0.030230  loss: 0.012870  eta: 0h3m  tot: 0h2m44s  (39.5%)98.1%  lr: 0.030160  loss: 0.012847  eta: 0h3m  tot: 0h2m44s  (39.6%)98.8%  lr: 0.030130  loss: 0.012853  eta: 0h3m  tot: 0h2m45s  (39.8%)99.1%  lr: 0.030110  loss: 0.012860  eta: 0h3m  tot: 0h2m45s  (39.8%)\n",
      " ---+++                Epoch    1 Train error : 0.01330207 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76.6%  lr: 0.022072  loss: 0.009234  eta: 0h3m  tot: 0h3m49s  (55.3%).4%  lr: 0.029920  loss: 0.006489  eta: 0h3m  tot: 0h2m47s  (40.1%)0.6%  lr: 0.029910  loss: 0.006475  eta: 0h3m  tot: 0h2m47s  (40.1%)2.0%  lr: 0.029780  loss: 0.009613  eta: 0h2m  tot: 0h2m48s  (40.4%)2.6%  lr: 0.029730  loss: 0.009764  eta: 0h3m  tot: 0h2m48s  (40.5%)2.7%  lr: 0.029710  loss: 0.009912  eta: 0h3m  tot: 0h2m48s  (40.5%)4.1%  lr: 0.029580  loss: 0.009554  eta: 0h3m  tot: 0h2m49s  (40.8%)5.5%  lr: 0.029429  loss: 0.009419  eta: 0h3m  tot: 0h2m50s  (41.1%)6.0%  lr: 0.029369  loss: 0.009116  eta: 0h3m  tot: 0h2m51s  (41.2%)6.7%  lr: 0.029329  loss: 0.009055  eta: 0h3m  tot: 0h2m51s  (41.3%)8.2%  lr: 0.029179  loss: 0.009257  eta: 0h3m  tot: 0h2m52s  (41.6%)8.4%  lr: 0.029179  loss: 0.009277  eta: 0h3m  tot: 0h2m52s  (41.7%)9.5%  lr: 0.029019  loss: 0.009351  eta: 0h3m  tot: 0h2m53s  (41.9%)9.5%  lr: 0.029009  loss: 0.009369  eta: 0h3m  tot: 0h2m53s  (41.9%)9.6%  lr: 0.028979  loss: 0.009401  eta: 0h3m  tot: 0h2m53s  (41.9%)10.1%  lr: 0.028949  loss: 0.009366  eta: 0h3m  tot: 0h2m54s  (42.0%)10.3%  lr: 0.028929  loss: 0.009272  eta: 0h3m  tot: 0h2m54s  (42.1%)10.4%  lr: 0.028929  loss: 0.009197  eta: 0h3m  tot: 0h2m54s  (42.1%)10.7%  lr: 0.028889  loss: 0.009226  eta: 0h3m  tot: 0h2m54s  (42.1%)10.9%  lr: 0.028879  loss: 0.009237  eta: 0h3m  tot: 0h2m54s  (42.2%)11.0%  lr: 0.028879  loss: 0.009252  eta: 0h3m  tot: 0h2m55s  (42.2%)11.6%  lr: 0.028769  loss: 0.009094  eta: 0h3m  tot: 0h2m55s  (42.3%)%  lr: 0.028769  loss: 0.009068  eta: 0h3m  tot: 0h2m55s  (42.3%)12.6%  lr: 0.028619  loss: 0.008996  eta: 0h3m  tot: 0h2m56s  (42.5%)12.9%  lr: 0.028599  loss: 0.009009  eta: 0h3m  tot: 0h2m57s  (42.6%)13.8%  lr: 0.028549  loss: 0.008839  eta: 0h3m  tot: 0h2m57s  (42.8%)14.5%  lr: 0.028408  loss: 0.008845  eta: 0h3m  tot: 0h2m58s  (42.9%)15.9%  lr: 0.028258  loss: 0.008689  eta: 0h3m  tot: 0h2m59s  (43.2%)16.3%  lr: 0.028258  loss: 0.008661  eta: 0h3m  tot: 0h2m59s  (43.3%)16.4%  lr: 0.028258  loss: 0.008665  eta: 0h3m  tot: 0h2m59s  (43.3%)16.5%  lr: 0.028228  loss: 0.008687  eta: 0h3m  tot: 0h2m59s  (43.3%)16.8%  lr: 0.028148  loss: 0.008656  eta: 0h3m  tot: 0h3m0s  (43.4%)16.9%  lr: 0.028138  loss: 0.008739  eta: 0h3m  tot: 0h3m0s  (43.4%)17.4%  lr: 0.028078  loss: 0.008760  eta: 0h3m  tot: 0h3m0s  (43.5%)17.8%  lr: 0.028038  loss: 0.008744  eta: 0h3m  tot: 0h3m0s  (43.6%)18.2%  lr: 0.027968  loss: 0.008786  eta: 0h3m  tot: 0h3m1s  (43.6%)18.7%  lr: 0.027938  loss: 0.008913  eta: 0h3m  tot: 0h3m1s  (43.7%)19.3%  lr: 0.027848  loss: 0.008897  eta: 0h3m  tot: 0h3m2s  (43.9%)19.6%  lr: 0.027798  loss: 0.008931  eta: 0h3m  tot: 0h3m2s  (43.9%)19.7%  lr: 0.027788  loss: 0.008902  eta: 0h3m  tot: 0h3m2s  (43.9%)19.9%  lr: 0.027788  loss: 0.008886  eta: 0h3m  tot: 0h3m2s  (44.0%)20.3%  lr: 0.027738  loss: 0.008911  eta: 0h3m  tot: 0h3m3s  (44.1%)21.2%  lr: 0.027658  loss: 0.008834  eta: 0h3m  tot: 0h3m3s  (44.2%)22.7%  lr: 0.027518  loss: 0.008885  eta: 0h3m  tot: 0h3m5s  (44.5%)23.8%  lr: 0.027417  loss: 0.008887  eta: 0h3m  tot: 0h3m6s  (44.8%)24.3%  lr: 0.027367  loss: 0.008924  eta: 0h3m  tot: 0h3m6s  (44.9%)24.5%  lr: 0.027347  loss: 0.008960  eta: 0h3m  tot: 0h3m6s  (44.9%)25.2%  lr: 0.027277  loss: 0.008936  eta: 0h3m  tot: 0h3m7s  (45.0%)26.2%  lr: 0.027147  loss: 0.008932  eta: 0h3m  tot: 0h3m8s  (45.2%)28.6%  lr: 0.026897  loss: 0.009055  eta: 0h3m  tot: 0h3m9s  (45.7%)28.9%  lr: 0.026887  loss: 0.009080  eta: 0h3m  tot: 0h3m10s  (45.8%)29.0%  lr: 0.026877  loss: 0.009074  eta: 0h3m  tot: 0h3m10s  (45.8%)29.3%  lr: 0.026857  loss: 0.009032  eta: 0h3m  tot: 0h3m10s  (45.9%)30.4%  lr: 0.026667  loss: 0.009034  eta: 0h3m  tot: 0h3m11s  (46.1%)31.5%  lr: 0.026547  loss: 0.009032  eta: 0h3m  tot: 0h3m11s  (46.3%)32.1%  lr: 0.026487  loss: 0.009054  eta: 0h3m  tot: 0h3m12s  (46.4%)33.9%  lr: 0.026316  loss: 0.009052  eta: 0h3m  tot: 0h3m14s  (46.8%)34.1%  lr: 0.026316  loss: 0.009043  eta: 0h3m  tot: 0h3m14s  (46.8%)35.6%  lr: 0.026206  loss: 0.009032  eta: 0h3m  tot: 0h3m15s  (47.1%)37.5%  lr: 0.026066  loss: 0.009099  eta: 0h3m  tot: 0h3m16s  (47.5%)37.9%  lr: 0.026016  loss: 0.009092  eta: 0h3m  tot: 0h3m16s  (47.6%)38.3%  lr: 0.026006  loss: 0.009090  eta: 0h3m  tot: 0h3m17s  (47.7%)38.5%  lr: 0.025956  loss: 0.009093  eta: 0h3m  tot: 0h3m17s  (47.7%)39.2%  lr: 0.025936  loss: 0.009134  eta: 0h3m  tot: 0h3m18s  (47.8%)39.5%  lr: 0.025926  loss: 0.009155  eta: 0h3m  tot: 0h3m18s  (47.9%)40.3%  lr: 0.025846  loss: 0.009142  eta: 0h3m  tot: 0h3m19s  (48.1%)40.4%  lr: 0.025836  loss: 0.009136  eta: 0h3m  tot: 0h3m19s  (48.1%)40.9%  lr: 0.025816  loss: 0.009156  eta: 0h3m  tot: 0h3m19s  (48.2%)41.3%  lr: 0.025756  loss: 0.009152  eta: 0h3m  tot: 0h3m19s  (48.3%)43.5%  lr: 0.025526  loss: 0.009110  eta: 0h3m  tot: 0h3m21s  (48.7%)%  lr: 0.025496  loss: 0.009107  eta: 0h3m  tot: 0h3m22s  (48.8%)44.3%  lr: 0.025476  loss: 0.009124  eta: 0h3m  tot: 0h3m22s  (48.9%)0.025476  loss: 0.009119  eta: 0h3m  tot: 0h3m22s  (48.9%)45.6%  lr: 0.025395  loss: 0.009101  eta: 0h3m  tot: 0h3m23s  (49.1%)46.5%  lr: 0.025295  loss: 0.009087  eta: 0h3m  tot: 0h3m24s  (49.3%)47.1%  lr: 0.025195  loss: 0.009066  eta: 0h3m  tot: 0h3m24s  (49.4%)47.5%  lr: 0.025155  loss: 0.009060  eta: 0h3m  tot: 0h3m25s  (49.5%)48.1%  lr: 0.025135  loss: 0.009063  eta: 0h3m  tot: 0h3m25s  (49.6%)48.2%  lr: 0.025125  loss: 0.009057  eta: 0h3m  tot: 0h3m26s  (49.6%)48.4%  lr: 0.025105  loss: 0.009082  eta: 0h3m  tot: 0h3m26s  (49.7%)49.3%  lr: 0.025005  loss: 0.009081  eta: 0h3m  tot: 0h3m27s  (49.9%)50.1%  lr: 0.024875  loss: 0.009054  eta: 0h3m  tot: 0h3m28s  (50.0%)50.7%  lr: 0.024825  loss: 0.009047  eta: 0h3m  tot: 0h3m28s  (50.1%)54.2%  lr: 0.024515  loss: 0.009147  eta: 0h3m  tot: 0h3m31s  (50.8%)55.2%  lr: 0.024394  loss: 0.009154  eta: 0h3m  tot: 0h3m32s  (51.0%)%  lr: 0.024334  loss: 0.009156  eta: 0h3m  tot: 0h3m32s  (51.1%)55.7%  lr: 0.024294  loss: 0.009194  eta: 0h3m  tot: 0h3m32s  (51.1%)55.8%  lr: 0.024294  loss: 0.009189  eta: 0h3m  tot: 0h3m32s  (51.2%)57.3%  lr: 0.024184  loss: 0.009211  eta: 0h3m  tot: 0h3m33s  (51.5%)57.7%  lr: 0.024154  loss: 0.009217  eta: 0h3m  tot: 0h3m33s  (51.5%)57.8%  lr: 0.024154  loss: 0.009215  eta: 0h3m  tot: 0h3m33s  (51.6%)58.8%  lr: 0.024044  loss: 0.009241  eta: 0h3m  tot: 0h3m34s  (51.8%)58.9%  lr: 0.024044  loss: 0.009241  eta: 0h3m  tot: 0h3m34s  (51.8%)59.9%  lr: 0.023944  loss: 0.009239  eta: 0h3m  tot: 0h3m35s  (52.0%)61.6%  lr: 0.023774  loss: 0.009215  eta: 0h3m  tot: 0h3m36s  (52.3%)61.8%  lr: 0.023754  loss: 0.009217  eta: 0h3m  tot: 0h3m37s  (52.4%)65.2%  lr: 0.023303  loss: 0.009237  eta: 0h3m  tot: 0h3m40s  (53.0%)66.3%  lr: 0.023203  loss: 0.009260  eta: 0h3m  tot: 0h3m41s  (53.3%)66.5%  lr: 0.023173  loss: 0.009259  eta: 0h3m  tot: 0h3m41s  (53.3%)66.8%  lr: 0.023123  loss: 0.009242  eta: 0h3m  tot: 0h3m41s  (53.4%)67.6%  lr: 0.023063  loss: 0.009249  eta: 0h3m  tot: 0h3m42s  (53.5%)67.9%  lr: 0.023033  loss: 0.009240  eta: 0h3m  tot: 0h3m43s  (53.6%)68.6%  lr: 0.022993  loss: 0.009286  eta: 0h3m  tot: 0h3m43s  (53.7%)69.0%  lr: 0.022973  loss: 0.009273  eta: 0h3m  tot: 0h3m44s  (53.8%)69.2%  lr: 0.022943  loss: 0.009263  eta: 0h3m  tot: 0h3m44s  (53.8%)70.1%  lr: 0.022833  loss: 0.009257  eta: 0h3m  tot: 0h3m44s  (54.0%)70.2%  lr: 0.022833  loss: 0.009249  eta: 0h3m  tot: 0h3m45s  (54.0%)70.6%  lr: 0.022833  loss: 0.009242  eta: 0h3m  tot: 0h3m45s  (54.1%)70.7%  lr: 0.022833  loss: 0.009243  eta: 0h3m  tot: 0h3m45s  (54.1%)%  lr: 0.022783  loss: 0.009235  eta: 0h3m  tot: 0h3m45s  (54.2%)  loss: 0.009248  eta: 0h3m  tot: 0h3m45s  (54.3%)71.8%  lr: 0.022683  loss: 0.009236  eta: 0h3m  tot: 0h3m46s  (54.4%)72.1%  lr: 0.022643  loss: 0.009227  eta: 0h3m  tot: 0h3m46s  (54.4%)%  lr: 0.022583  loss: 0.009223  eta: 0h3m  tot: 0h3m46s  (54.5%)74.5%  lr: 0.022352  loss: 0.009195  eta: 0h3m  tot: 0h3m48s  (54.9%)74.7%  lr: 0.022322  loss: 0.009184  eta: 0h3m  tot: 0h3m48s  (54.9%)74.8%  lr: 0.022322  loss: 0.009192  eta: 0h3m  tot: 0h3m48s  (55.0%)74.9%  lr: 0.022312  loss: 0.009197  eta: 0h3m  tot: 0h3m48s  (55.0%)76.2%  lr: 0.022132  loss: 0.009232  eta: 0h3m  tot: 0h3m49s  (55.2%)76.7%  lr: 0.022052  loss: 0.009233  eta: 0h3m  tot: 0h3m50s  (55.3%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020010  loss: 0.009367  eta: 0h2m  tot: 0h4m8s  (60.0%)76.9%  lr: 0.022042  loss: 0.009244  eta: 0h3m  tot: 0h3m50s  (55.4%)77.0%  lr: 0.022022  loss: 0.009237  eta: 0h3m  tot: 0h3m50s  (55.4%)79.1%  lr: 0.021822  loss: 0.009228  eta: 0h3m  tot: 0h3m52s  (55.8%)79.4%  lr: 0.021802  loss: 0.009227  eta: 0h3m  tot: 0h3m52s  (55.9%)79.5%  lr: 0.021792  loss: 0.009221  eta: 0h3m  tot: 0h3m52s  (55.9%)80.8%  lr: 0.021682  loss: 0.009245  eta: 0h3m  tot: 0h3m53s  (56.2%)81.2%  lr: 0.021652  loss: 0.009244  eta: 0h2m  tot: 0h3m53s  (56.2%)82.2%  lr: 0.021562  loss: 0.009262  eta: 0h2m  tot: 0h3m54s  (56.4%)%  lr: 0.021522  loss: 0.009263  eta: 0h2m  tot: 0h3m54s  (56.5%)82.7%  lr: 0.021502  loss: 0.009257  eta: 0h2m  tot: 0h3m54s  (56.5%)82.8%  lr: 0.021492  loss: 0.009263  eta: 0h2m  tot: 0h3m54s  (56.6%)83.1%  lr: 0.021452  loss: 0.009265  eta: 0h2m  tot: 0h3m55s  (56.6%)83.9%  lr: 0.021361  loss: 0.009259  eta: 0h2m  tot: 0h3m55s  (56.8%)84.7%  lr: 0.021261  loss: 0.009264  eta: 0h2m  tot: 0h3m56s  (56.9%)85.8%  lr: 0.021211  loss: 0.009263  eta: 0h2m  tot: 0h3m57s  (57.2%)85.9%  lr: 0.021211  loss: 0.009260  eta: 0h2m  tot: 0h3m57s  (57.2%)87.5%  lr: 0.021061  loss: 0.009257  eta: 0h2m  tot: 0h3m58s  (57.5%)87.7%  lr: 0.021041  loss: 0.009264  eta: 0h2m  tot: 0h3m59s  (57.5%)88.7%  lr: 0.020961  loss: 0.009264  eta: 0h2m  tot: 0h4m0s  (57.7%)89.2%  lr: 0.020931  loss: 0.009264  eta: 0h2m  tot: 0h4m0s  (57.8%)89.7%  lr: 0.020891  loss: 0.009281  eta: 0h2m  tot: 0h4m0s  (57.9%)89.8%  lr: 0.020851  loss: 0.009288  eta: 0h2m  tot: 0h4m0s  (58.0%)%  lr: 0.020841  loss: 0.009295  eta: 0h2m  tot: 0h4m1s  (58.0%)91.2%  lr: 0.020691  loss: 0.009310  eta: 0h2m  tot: 0h4m2s  (58.2%)91.9%  lr: 0.020651  loss: 0.009305  eta: 0h2m  tot: 0h4m2s  (58.4%)93.0%  lr: 0.020591  loss: 0.009325  eta: 0h2m  tot: 0h4m3s  (58.6%)93.4%  lr: 0.020501  loss: 0.009324  eta: 0h2m  tot: 0h4m4s  (58.7%)93.8%  lr: 0.020461  loss: 0.009328  eta: 0h2m  tot: 0h4m4s  (58.8%)94.0%  lr: 0.020441  loss: 0.009325  eta: 0h2m  tot: 0h4m4s  (58.8%)94.4%  lr: 0.020381  loss: 0.009324  eta: 0h2m  tot: 0h4m5s  (58.9%)94.6%  lr: 0.020361  loss: 0.009316  eta: 0h2m  tot: 0h4m5s  (58.9%)94.8%  lr: 0.020340  loss: 0.009319  eta: 0h2m  tot: 0h4m5s  (59.0%)96.0%  lr: 0.020210  loss: 0.009347  eta: 0h2m  tot: 0h4m6s  (59.2%)96.2%  lr: 0.020180  loss: 0.009349  eta: 0h2m  tot: 0h4m6s  (59.2%)96.3%  lr: 0.020170  loss: 0.009346  eta: 0h2m  tot: 0h4m6s  (59.3%)96.4%  lr: 0.020150  loss: 0.009349  eta: 0h2m  tot: 0h4m6s  (59.3%)96.5%  lr: 0.020140  loss: 0.009351  eta: 0h2m  tot: 0h4m6s  (59.3%)0h2m  tot: 0h4m6s  (59.4%)  loss: 0.009367  eta: 0h2m  tot: 0h4m7s  (59.6%)98.0%  lr: 0.020050  loss: 0.009366  eta: 0h2m  tot: 0h4m7s  (59.6%)98.2%  lr: 0.020050  loss: 0.009369  eta: 0h2m  tot: 0h4m7s  (59.6%)\n",
      " ---+++                Epoch    2 Train error : 0.00944269 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82.6%  lr: 0.011201  loss: 0.007890  eta: 0h1m  tot: 0h5m16s  (76.5%)6%  lr: 0.019840  loss: 0.009089  eta: 0h1m  tot: 0h4m9s  (60.3%)1.8%  lr: 0.019810  loss: 0.008790  eta: 0h1m  tot: 0h4m9s  (60.4%)1.9%  lr: 0.019810  loss: 0.008795  eta: 0h1m  tot: 0h4m9s  (60.4%)2.7%  lr: 0.019770  loss: 0.008170  eta: 0h1m  tot: 0h4m10s  (60.5%)3.3%  lr: 0.019710  loss: 0.008075  eta: 0h2m  tot: 0h4m10s  (60.7%)3.6%  lr: 0.019670  loss: 0.008200  eta: 0h2m  tot: 0h4m11s  (60.7%)4.6%  lr: 0.019570  loss: 0.008257  eta: 0h2m  tot: 0h4m11s  (60.9%)5.4%  lr: 0.019459  loss: 0.008387  eta: 0h2m  tot: 0h4m12s  (61.1%)6.7%  lr: 0.019339  loss: 0.008300  eta: 0h2m  tot: 0h4m13s  (61.3%)7.6%  lr: 0.019279  loss: 0.008164  eta: 0h2m  tot: 0h4m14s  (61.5%)9.7%  lr: 0.019089  loss: 0.008351  eta: 0h2m  tot: 0h4m15s  (61.9%)9.9%  lr: 0.019079  loss: 0.008349  eta: 0h2m  tot: 0h4m15s  (62.0%)10.2%  lr: 0.019049  loss: 0.008412  eta: 0h2m  tot: 0h4m15s  (62.0%)11.3%  lr: 0.018949  loss: 0.008493  eta: 0h2m  tot: 0h4m16s  (62.3%)11.5%  lr: 0.018929  loss: 0.008527  eta: 0h2m  tot: 0h4m16s  (62.3%)11.8%  lr: 0.018919  loss: 0.008450  eta: 0h2m  tot: 0h4m16s  (62.4%)%  lr: 0.018759  loss: 0.008409  eta: 0h2m  tot: 0h4m18s  (62.6%)14.3%  lr: 0.018659  loss: 0.008325  eta: 0h2m  tot: 0h4m19s  (62.9%)14.4%  lr: 0.018649  loss: 0.008352  eta: 0h2m  tot: 0h4m19s  (62.9%)15.3%  lr: 0.018448  loss: 0.008306  eta: 0h2m  tot: 0h4m20s  (63.1%)%  lr: 0.018278  loss: 0.008249  eta: 0h2m  tot: 0h4m21s  (63.3%)16.5%  lr: 0.018278  loss: 0.008209  eta: 0h2m  tot: 0h4m21s  (63.3%)17.2%  lr: 0.018218  loss: 0.008278  eta: 0h2m  tot: 0h4m22s  (63.4%)17.8%  lr: 0.018158  loss: 0.008255  eta: 0h2m  tot: 0h4m22s  (63.6%)17.9%  lr: 0.018138  loss: 0.008221  eta: 0h2m  tot: 0h4m22s  (63.6%)18.5%  lr: 0.018048  loss: 0.008282  eta: 0h2m  tot: 0h4m23s  (63.7%)19.8%  lr: 0.017918  loss: 0.008247  eta: 0h2m  tot: 0h4m24s  (64.0%)%  lr: 0.017868  loss: 0.008234  eta: 0h2m  tot: 0h4m24s  (64.0%)20.6%  lr: 0.017848  loss: 0.008245  eta: 0h2m  tot: 0h4m25s  (64.1%)21.0%  lr: 0.017798  loss: 0.008254  eta: 0h2m  tot: 0h4m25s  (64.2%)21.2%  lr: 0.017778  loss: 0.008276  eta: 0h2m  tot: 0h4m25s  (64.2%)21.4%  lr: 0.017728  loss: 0.008271  eta: 0h2m  tot: 0h4m25s  (64.3%)22.3%  lr: 0.017628  loss: 0.008257  eta: 0h2m  tot: 0h4m26s  (64.5%)24.1%  lr: 0.017508  loss: 0.008236  eta: 0h2m  tot: 0h4m28s  (64.8%)%  lr: 0.017417  loss: 0.008159  eta: 0h2m  tot: 0h4m28s  (65.1%)25.6%  lr: 0.017417  loss: 0.008147  eta: 0h2m  tot: 0h4m28s  (65.1%)25.7%  lr: 0.017407  loss: 0.008185  eta: 0h2m  tot: 0h4m28s  (65.1%)26.6%  lr: 0.017257  loss: 0.008191  eta: 0h2m  tot: 0h4m29s  (65.3%)27.5%  lr: 0.017177  loss: 0.008161  eta: 0h2m  tot: 0h4m30s  (65.5%)28.0%  lr: 0.017127  loss: 0.008168  eta: 0h2m  tot: 0h4m31s  (65.6%)28.8%  lr: 0.017067  loss: 0.008181  eta: 0h2m  tot: 0h4m31s  (65.8%)30.2%  lr: 0.017007  loss: 0.008095  eta: 0h2m  tot: 0h4m32s  (66.0%)30.7%  lr: 0.016957  loss: 0.008119  eta: 0h2m  tot: 0h4m32s  (66.1%)31.1%  lr: 0.016897  loss: 0.008151  eta: 0h2m  tot: 0h4m33s  (66.2%)31.4%  lr: 0.016857  loss: 0.008148  eta: 0h2m  tot: 0h4m33s  (66.3%)32.0%  lr: 0.016807  loss: 0.008098  eta: 0h2m  tot: 0h4m34s  (66.4%)32.1%  lr: 0.016807  loss: 0.008082  eta: 0h2m  tot: 0h4m34s  (66.4%)32.6%  lr: 0.016767  loss: 0.008100  eta: 0h2m  tot: 0h4m34s  (66.5%)34.6%  lr: 0.016517  loss: 0.008094  eta: 0h2m  tot: 0h4m36s  (66.9%)35.4%  lr: 0.016447  loss: 0.008046  eta: 0h2m  tot: 0h4m36s  (67.1%)35.9%  lr: 0.016316  loss: 0.008022  eta: 0h2m  tot: 0h4m37s  (67.2%)36.0%  lr: 0.016306  loss: 0.008017  eta: 0h2m  tot: 0h4m37s  (67.2%)38.5%  lr: 0.016056  loss: 0.007996  eta: 0h2m  tot: 0h4m39s  (67.7%)%  lr: 0.016006  loss: 0.007977  eta: 0h2m  tot: 0h4m39s  (67.8%)39.9%  lr: 0.015886  loss: 0.008018  eta: 0h2m  tot: 0h4m40s  (68.0%)40.5%  lr: 0.015856  loss: 0.008008  eta: 0h2m  tot: 0h4m41s  (68.1%)41.7%  lr: 0.015736  loss: 0.007954  eta: 0h2m  tot: 0h4m42s  (68.3%)43.9%  lr: 0.015466  loss: 0.007956  eta: 0h2m  tot: 0h4m43s  (68.8%)44.4%  lr: 0.015365  loss: 0.007979  eta: 0h2m  tot: 0h4m43s  (68.9%)44.6%  lr: 0.015335  loss: 0.007973  eta: 0h2m  tot: 0h4m44s  (68.9%)45.6%  lr: 0.015185  loss: 0.007949  eta: 0h2m  tot: 0h4m44s  (69.1%)45.6%  lr: 0.015185  loss: 0.007945  eta: 0h2m  tot: 0h4m44s  (69.1%)46.5%  lr: 0.015115  loss: 0.007979  eta: 0h2m  tot: 0h4m45s  (69.3%)46.9%  lr: 0.015095  loss: 0.007991  eta: 0h2m  tot: 0h4m45s  (69.4%)  loss: 0.007985  eta: 0h2m  tot: 0h4m45s  (69.4%)48.3%  lr: 0.014975  loss: 0.008020  eta: 0h2m  tot: 0h4m47s  (69.7%)48.7%  lr: 0.014965  loss: 0.008007  eta: 0h2m  tot: 0h4m47s  (69.7%)50.0%  lr: 0.014855  loss: 0.008039  eta: 0h2m  tot: 0h4m48s  (70.0%)50.7%  lr: 0.014775  loss: 0.008034  eta: 0h2m  tot: 0h4m49s  (70.1%)50.9%  lr: 0.014735  loss: 0.008050  eta: 0h2m  tot: 0h4m49s  (70.2%)51.0%  lr: 0.014685  loss: 0.008056  eta: 0h2m  tot: 0h4m49s  (70.2%)52.4%  lr: 0.014515  loss: 0.008104  eta: 0h1m  tot: 0h4m50s  (70.5%)52.8%  lr: 0.014485  loss: 0.008097  eta: 0h1m  tot: 0h4m51s  (70.6%)53.1%  lr: 0.014455  loss: 0.008089  eta: 0h1m  tot: 0h4m51s  (70.6%)53.5%  lr: 0.014394  loss: 0.008088  eta: 0h1m  tot: 0h4m51s  (70.7%)53.8%  lr: 0.014364  loss: 0.008106  eta: 0h1m  tot: 0h4m52s  (70.8%)54.6%  lr: 0.014304  loss: 0.008073  eta: 0h1m  tot: 0h4m52s  (70.9%)55.5%  lr: 0.014174  loss: 0.008063  eta: 0h1m  tot: 0h4m53s  (71.1%)56.1%  lr: 0.014114  loss: 0.008078  eta: 0h1m  tot: 0h4m54s  (71.2%)56.5%  lr: 0.014064  loss: 0.008087  eta: 0h1m  tot: 0h4m54s  (71.3%)56.8%  lr: 0.014054  loss: 0.008099  eta: 0h1m  tot: 0h4m54s  (71.4%)57.1%  lr: 0.014024  loss: 0.008118  eta: 0h1m  tot: 0h4m55s  (71.4%)57.2%  lr: 0.014024  loss: 0.008116  eta: 0h1m  tot: 0h4m55s  (71.4%)59.5%  lr: 0.013744  loss: 0.008091  eta: 0h1m  tot: 0h4m57s  (71.9%)60.0%  lr: 0.013654  loss: 0.008088  eta: 0h1m  tot: 0h4m57s  (72.0%)60.6%  lr: 0.013544  loss: 0.008079  eta: 0h1m  tot: 0h4m58s  (72.1%)62.5%  lr: 0.013434  loss: 0.008076  eta: 0h1m  tot: 0h4m59s  (72.5%)63.1%  lr: 0.013373  loss: 0.008067  eta: 0h1m  tot: 0h4m59s  (72.6%)64.3%  lr: 0.013223  loss: 0.008061  eta: 0h1m  tot: 0h5m1s  (72.9%)64.9%  lr: 0.013163  loss: 0.008043  eta: 0h1m  tot: 0h5m1s  (73.0%)65.4%  lr: 0.013123  loss: 0.008025  eta: 0h1m  tot: 0h5m1s  (73.1%)68.1%  lr: 0.012903  loss: 0.007986  eta: 0h1m  tot: 0h5m3s  (73.6%)69.2%  lr: 0.012783  loss: 0.007986  eta: 0h1m  tot: 0h5m4s  (73.8%)69.4%  lr: 0.012773  loss: 0.007997  eta: 0h1m  tot: 0h5m4s  (73.9%)70.1%  lr: 0.012703  loss: 0.007991  eta: 0h1m  tot: 0h5m5s  (74.0%)70.2%  lr: 0.012703  loss: 0.008000  eta: 0h1m  tot: 0h5m5s  (74.0%)70.4%  lr: 0.012663  loss: 0.007995  eta: 0h1m  tot: 0h5m5s  (74.1%)70.6%  lr: 0.012663  loss: 0.007992  eta: 0h1m  tot: 0h5m5s  (74.1%)70.8%  lr: 0.012653  loss: 0.007989  eta: 0h1m  tot: 0h5m5s  (74.2%)72.7%  lr: 0.012393  loss: 0.007982  eta: 0h1m  tot: 0h5m7s  (74.5%)73.4%  lr: 0.012322  loss: 0.007993  eta: 0h1m  tot: 0h5m7s  (74.7%)73.7%  lr: 0.012272  loss: 0.007983  eta: 0h1m  tot: 0h5m8s  (74.7%)%  lr: 0.012222  loss: 0.007981  eta: 0h1m  tot: 0h5m8s  (74.8%)%  lr: 0.012202  loss: 0.007993  eta: 0h1m  tot: 0h5m8s  (74.9%)75.2%  lr: 0.012102  loss: 0.007980  eta: 0h1m  tot: 0h5m9s  (75.0%)75.3%  lr: 0.012082  loss: 0.007983  eta: 0h1m  tot: 0h5m9s  (75.1%)0.012012  loss: 0.007948  eta: 0h1m  tot: 0h5m10s  (75.2%)76.2%  lr: 0.012002  loss: 0.007945  eta: 0h1m  tot: 0h5m10s  (75.2%)78.7%  lr: 0.011712  loss: 0.007939  eta: 0h1m  tot: 0h5m12s  (75.7%)78.9%  lr: 0.011682  loss: 0.007936  eta: 0h1m  tot: 0h5m12s  (75.8%)79.1%  lr: 0.011662  loss: 0.007925  eta: 0h1m  tot: 0h5m12s  (75.8%)79.5%  lr: 0.011572  loss: 0.007913  eta: 0h1m  tot: 0h5m13s  (75.9%)79.7%  lr: 0.011562  loss: 0.007908  eta: 0h1m  tot: 0h5m13s  (75.9%)80.1%  lr: 0.011492  loss: 0.007914  eta: 0h1m  tot: 0h5m13s  (76.0%)80.9%  lr: 0.011432  loss: 0.007910  eta: 0h1m  tot: 0h5m14s  (76.2%)81.7%  lr: 0.011311  loss: 0.007894  eta: 0h1m  tot: 0h5m15s  (76.3%)82.3%  lr: 0.011251  loss: 0.007898  eta: 0h1m  tot: 0h5m15s  (76.5%)82.4%  lr: 0.011241  loss: 0.007893  eta: 0h1m  tot: 0h5m16s  (76.5%)0.007897  eta: 0h1m  tot: 0h5m16s  (76.5%)82.6%  lr: 0.011181  loss: 0.007886  eta: 0h1m  tot: 0h5m16s  (76.5%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010000  loss: 0.007797  eta: 0h1m  tot: 0h5m26s  (80.0%)h1m  tot: 0h5m16s  (76.6%)83.8%  lr: 0.011081  loss: 0.007876  eta: 0h1m  tot: 0h5m17s  (76.8%)84.4%  lr: 0.011041  loss: 0.007872  eta: 0h1m  tot: 0h5m18s  (76.9%)84.6%  lr: 0.011031  loss: 0.007865  eta: 0h1m  tot: 0h5m18s  (76.9%)84.7%  lr: 0.011031  loss: 0.007860  eta: 0h1m  tot: 0h5m18s  (76.9%)85.1%  lr: 0.011001  loss: 0.007870  eta: 0h1m  tot: 0h5m18s  (77.0%)86.7%  lr: 0.010931  loss: 0.007862  eta: 0h1m  tot: 0h5m19s  (77.3%)87.1%  lr: 0.010881  loss: 0.007858  eta: 0h1m  tot: 0h5m20s  (77.4%)88.0%  lr: 0.010831  loss: 0.007855  eta: 0h1m  tot: 0h5m20s  (77.6%)88.1%  lr: 0.010801  loss: 0.007858  eta: 0h1m  tot: 0h5m21s  (77.6%)88.6%  lr: 0.010731  loss: 0.007860  eta: 0h1m  tot: 0h5m21s  (77.7%)90.9%  lr: 0.010531  loss: 0.007852  eta: 0h1m  tot: 0h5m22s  (78.2%)91.3%  lr: 0.010501  loss: 0.007848  eta: 0h1m  tot: 0h5m22s  (78.3%)91.4%  lr: 0.010481  loss: 0.007843  eta: 0h1m  tot: 0h5m23s  (78.3%)91.5%  lr: 0.010481  loss: 0.007841  eta: 0h1m  tot: 0h5m23s  (78.3%)91.9%  lr: 0.010431  loss: 0.007831  eta: 0h1m  tot: 0h5m23s  (78.4%)92.6%  lr: 0.010361  loss: 0.007852  eta: 0h1m  tot: 0h5m23s  (78.5%)93.0%  lr: 0.010310  loss: 0.007855  eta: 0h1m  tot: 0h5m24s  (78.6%)93.1%  lr: 0.010300  loss: 0.007851  eta: 0h1m  tot: 0h5m24s  (78.6%)93.6%  lr: 0.010220  loss: 0.007841  eta: 0h1m  tot: 0h5m24s  (78.7%)93.7%  lr: 0.010200  loss: 0.007839  eta: 0h1m  tot: 0h5m24s  (78.7%)\n",
      " ---+++                Epoch    3 Train error : 0.00775283 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78.7%  lr: 0.001872  loss: 0.007150  eta: <1min   tot: 0h6m30s  (95.7%).5%  lr: 0.009740  loss: 0.007423  eta: <1min   tot: 0h5m28s  (80.7%)4.2%  lr: 0.009660  loss: 0.007202  eta: <1min   tot: 0h5m29s  (80.8%)5.6%  lr: 0.009580  loss: 0.007470  eta: <1min   tot: 0h5m29s  (81.1%)7.5%  lr: 0.009419  loss: 0.007625  eta: <1min   tot: 0h5m30s  (81.5%)9.5%  lr: 0.009139  loss: 0.007529  eta: <1min   tot: 0h5m31s  (81.9%)9.7%  lr: 0.009139  loss: 0.007532  eta: <1min   tot: 0h5m32s  (81.9%)11.6%  lr: 0.008879  loss: 0.007407  eta: <1min   tot: 0h5m33s  (82.3%)11.8%  lr: 0.008869  loss: 0.007434  eta: <1min   tot: 0h5m33s  (82.4%)12.8%  lr: 0.008759  loss: 0.007403  eta: <1min   tot: 0h5m34s  (82.6%)12.9%  lr: 0.008759  loss: 0.007368  eta: <1min   tot: 0h5m34s  (82.6%)13.8%  lr: 0.008679  loss: 0.007398  eta: <1min   tot: 0h5m34s  (82.8%)14.1%  lr: 0.008659  loss: 0.007447  eta: <1min   tot: 0h5m34s  (82.8%)%  lr: 0.008438  loss: 0.007480  eta: <1min   tot: 0h5m37s  (83.4%)17.8%  lr: 0.008348  loss: 0.007449  eta: <1min   tot: 0h5m37s  (83.6%)19.0%  lr: 0.008268  loss: 0.007541  eta: <1min   tot: 0h5m38s  (83.8%)19.3%  lr: 0.008248  loss: 0.007571  eta: <1min   tot: 0h5m38s  (83.9%)20.4%  lr: 0.008118  loss: 0.007456  eta: <1min   tot: 0h5m39s  (84.1%)21.4%  lr: 0.008038  loss: 0.007462  eta: <1min   tot: 0h5m40s  (84.3%)22.0%  lr: 0.007988  loss: 0.007448  eta: <1min   tot: 0h5m40s  (84.4%)22.7%  lr: 0.007938  loss: 0.007418  eta: <1min   tot: 0h5m41s  (84.5%)%  lr: 0.007738  loss: 0.007373  eta: <1min   tot: 0h5m42s  (84.8%)25.0%  lr: 0.007678  loss: 0.007310  eta: <1min   tot: 0h5m43s  (85.0%)25.1%  lr: 0.007678  loss: 0.007300  eta: <1min   tot: 0h5m43s  (85.0%)25.3%  lr: 0.007638  loss: 0.007267  eta: <1min   tot: 0h5m43s  (85.1%)25.4%  lr: 0.007638  loss: 0.007283  eta: <1min   tot: 0h5m44s  (85.1%)25.6%  lr: 0.007628  loss: 0.007267  eta: <1min   tot: 0h5m44s  (85.1%)27.0%  lr: 0.007518  loss: 0.007316  eta: <1min   tot: 0h5m45s  (85.4%)%  lr: 0.007457  loss: 0.007284  eta: <1min   tot: 0h5m46s  (85.5%)28.8%  lr: 0.007357  loss: 0.007259  eta: <1min   tot: 0h5m47s  (85.8%)29.2%  lr: 0.007267  loss: 0.007245  eta: <1min   tot: 0h5m47s  (85.8%)30.4%  lr: 0.007097  loss: 0.007217  eta: <1min   tot: 0h5m48s  (86.1%)30.9%  lr: 0.007027  loss: 0.007180  eta: <1min   tot: 0h5m48s  (86.2%)31.1%  lr: 0.006987  loss: 0.007162  eta: <1min   tot: 0h5m49s  (86.2%)32.0%  lr: 0.006897  loss: 0.007100  eta: <1min   tot: 0h5m49s  (86.4%)32.9%  lr: 0.006747  loss: 0.007157  eta: <1min   tot: 0h5m50s  (86.6%)33.2%  lr: 0.006697  loss: 0.007166  eta: <1min   tot: 0h5m50s  (86.6%)33.7%  lr: 0.006647  loss: 0.007178  eta: <1min   tot: 0h5m51s  (86.7%)33.8%  lr: 0.006647  loss: 0.007181  eta: <1min   tot: 0h5m51s  (86.8%)34.3%  lr: 0.006567  loss: 0.007166  eta: <1min   tot: 0h5m51s  (86.9%)34.5%  lr: 0.006567  loss: 0.007194  eta: <1min   tot: 0h5m52s  (86.9%)34.6%  lr: 0.006557  loss: 0.007184  eta: <1min   tot: 0h5m52s  (86.9%)34.9%  lr: 0.006497  loss: 0.007191  eta: <1min   tot: 0h5m52s  (87.0%)36.6%  lr: 0.006296  loss: 0.007211  eta: <1min   tot: 0h5m54s  (87.3%)%  lr: 0.006286  loss: 0.007197  eta: <1min   tot: 0h5m54s  (87.4%)37.0%  lr: 0.006276  loss: 0.007204  eta: <1min   tot: 0h5m54s  (87.4%)38.8%  lr: 0.006106  loss: 0.007167  eta: <1min   tot: 0h5m56s  (87.8%)39.5%  lr: 0.006036  loss: 0.007161  eta: <1min   tot: 0h5m56s  (87.9%)39.7%  lr: 0.006026  loss: 0.007146  eta: <1min   tot: 0h5m56s  (87.9%)40.1%  lr: 0.005976  loss: 0.007159  eta: <1min   tot: 0h5m57s  (88.0%)%  lr: 0.005846  loss: 0.007137  eta: <1min   tot: 0h5m58s  (88.3%)41.9%  lr: 0.005796  loss: 0.007127  eta: <1min   tot: 0h5m58s  (88.4%)42.0%  lr: 0.005786  loss: 0.007123  eta: <1min   tot: 0h5m58s  (88.4%)44.1%  lr: 0.005566  loss: 0.007248  eta: <1min   tot: 0h6m0s  (88.8%)44.2%  lr: 0.005566  loss: 0.007255  eta: <1min   tot: 0h6m0s  (88.8%)%  lr: 0.005496  loss: 0.007265  eta: <1min   tot: 0h6m1s  (88.9%)45.4%  lr: 0.005415  loss: 0.007233  eta: <1min   tot: 0h6m1s  (89.1%)45.6%  lr: 0.005355  loss: 0.007247  eta: <1min   tot: 0h6m1s  (89.1%)  loss: 0.007254  eta: <1min   tot: 0h6m1s  (89.1%)46.4%  lr: 0.005315  loss: 0.007231  eta: <1min   tot: 0h6m2s  (89.3%)46.5%  lr: 0.005305  loss: 0.007221  eta: <1min   tot: 0h6m2s  (89.3%)%  lr: 0.005305  loss: 0.007224  eta: <1min   tot: 0h6m2s  (89.3%)47.0%  lr: 0.005285  loss: 0.007237  eta: <1min   tot: 0h6m3s  (89.4%)47.3%  lr: 0.005285  loss: 0.007239  eta: <1min   tot: 0h6m3s  (89.5%)48.9%  lr: 0.005095  loss: 0.007181  eta: <1min   tot: 0h6m4s  (89.8%)49.7%  lr: 0.004955  loss: 0.007163  eta: <1min   tot: 0h6m5s  (89.9%)50.7%  lr: 0.004935  loss: 0.007145  eta: <1min   tot: 0h6m6s  (90.1%)50.9%  lr: 0.004895  loss: 0.007158  eta: <1min   tot: 0h6m6s  (90.2%)51.7%  lr: 0.004835  loss: 0.007189  eta: <1min   tot: 0h6m6s  (90.3%)52.4%  lr: 0.004715  loss: 0.007166  eta: <1min   tot: 0h6m7s  (90.5%)52.8%  lr: 0.004645  loss: 0.007185  eta: <1min   tot: 0h6m8s  (90.6%)0.007165  eta: <1min   tot: 0h6m9s  (90.8%)54.0%  lr: 0.004435  loss: 0.007181  eta: <1min   tot: 0h6m9s  (90.8%)54.3%  lr: 0.004404  loss: 0.007187  eta: <1min   tot: 0h6m9s  (90.9%)55.5%  lr: 0.004304  loss: 0.007209  eta: <1min   tot: 0h6m10s  (91.1%)56.2%  lr: 0.004184  loss: 0.007216  eta: <1min   tot: 0h6m10s  (91.2%)%  lr: 0.004124  loss: 0.007207  eta: <1min   tot: 0h6m11s  (91.3%)57.0%  lr: 0.004104  loss: 0.007219  eta: <1min   tot: 0h6m11s  (91.4%)58.1%  lr: 0.004014  loss: 0.007214  eta: <1min   tot: 0h6m12s  (91.6%)m13s  (91.9%)59.5%  lr: 0.003914  loss: 0.007228  eta: <1min   tot: 0h6m13s  (91.9%)61.6%  lr: 0.003744  loss: 0.007211  eta: <1min   tot: 0h6m14s  (92.3%)61.9%  lr: 0.003684  loss: 0.007212  eta: <1min   tot: 0h6m14s  (92.4%)62.7%  lr: 0.003634  loss: 0.007204  eta: <1min   tot: 0h6m15s  (92.5%)63.6%  lr: 0.003584  loss: 0.007188  eta: <1min   tot: 0h6m15s  (92.7%)64.3%  lr: 0.003534  loss: 0.007223  eta: <1min   tot: 0h6m16s  (92.9%)65.2%  lr: 0.003444  loss: 0.007232  eta: <1min   tot: 0h6m17s  (93.0%)65.6%  lr: 0.003373  loss: 0.007214  eta: <1min   tot: 0h6m17s  (93.1%)66.2%  lr: 0.003343  loss: 0.007184  eta: <1min   tot: 0h6m18s  (93.2%)67.3%  lr: 0.003183  loss: 0.007163  eta: <1min   tot: 0h6m19s  (93.5%)68.1%  lr: 0.003083  loss: 0.007166  eta: <1min   tot: 0h6m20s  (93.6%)68.8%  lr: 0.003043  loss: 0.007175  eta: <1min   tot: 0h6m20s  (93.8%)69.1%  lr: 0.002993  loss: 0.007200  eta: <1min   tot: 0h6m21s  (93.8%)71.0%  lr: 0.002633  loss: 0.007194  eta: <1min   tot: 0h6m22s  (94.2%)71.1%  lr: 0.002623  loss: 0.007192  eta: <1min   tot: 0h6m22s  (94.2%)71.3%  lr: 0.002613  loss: 0.007198  eta: <1min   tot: 0h6m23s  (94.3%)71.6%  lr: 0.002593  loss: 0.007206  eta: <1min   tot: 0h6m23s  (94.3%)71.7%  lr: 0.002583  loss: 0.007202  eta: <1min   tot: 0h6m23s  (94.3%)71.9%  lr: 0.002583  loss: 0.007194  eta: <1min   tot: 0h6m23s  (94.4%)72.2%  lr: 0.002553  loss: 0.007181  eta: <1min   tot: 0h6m23s  (94.4%)72.6%  lr: 0.002463  loss: 0.007183  eta: <1min   tot: 0h6m24s  (94.5%)72.8%  lr: 0.002453  loss: 0.007198  eta: <1min   tot: 0h6m24s  (94.6%)73.0%  lr: 0.002443  loss: 0.007187  eta: <1min   tot: 0h6m24s  (94.6%)73.1%  lr: 0.002443  loss: 0.007192  eta: <1min   tot: 0h6m24s  (94.6%)  eta: <1min   tot: 0h6m25s  (94.7%)73.5%  lr: 0.002423  loss: 0.007196  eta: <1min   tot: 0h6m25s  (94.7%)73.9%  lr: 0.002382  loss: 0.007191  eta: <1min   tot: 0h6m25s  (94.8%)74.3%  lr: 0.002342  loss: 0.007203  eta: <1min   tot: 0h6m26s  (94.9%)74.4%  lr: 0.002312  loss: 0.007201  eta: <1min   tot: 0h6m26s  (94.9%)%  lr: 0.002312  loss: 0.007206  eta: <1min   tot: 0h6m26s  (94.9%)74.5%  lr: 0.002312  loss: 0.007213  eta: <1min   tot: 0h6m26s  (94.9%)75.1%  lr: 0.002262  loss: 0.007201  eta: <1min   tot: 0h6m27s  (95.0%)75.2%  lr: 0.002242  loss: 0.007198  eta: <1min   tot: 0h6m27s  (95.0%)75.3%  lr: 0.002212  loss: 0.007195  eta: <1min   tot: 0h6m27s  (95.1%)75.5%  lr: 0.002202  loss: 0.007187  eta: <1min   tot: 0h6m27s  (95.1%)75.9%  lr: 0.002102  loss: 0.007195  eta: <1min   tot: 0h6m28s  (95.2%)76.1%  lr: 0.002072  loss: 0.007189  eta: <1min   tot: 0h6m28s  (95.2%)78.3%  lr: 0.001922  loss: 0.007164  eta: <1min   tot: 0h6m29s  (95.7%)78.8%  lr: 0.001862  loss: 0.007155  eta: <1min   tot: 0h6m30s  (95.8%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000000  loss: 0.007120  eta: <1min   tot: 0h6m48s  (100.0%).3%  lr: 0.001762  loss: 0.007147  eta: <1min   tot: 0h6m30s  (95.9%)80.0%  lr: 0.001662  loss: 0.007172  eta: <1min   tot: 0h6m31s  (96.0%)80.2%  lr: 0.001642  loss: 0.007168  eta: <1min   tot: 0h6m31s  (96.0%)80.4%  lr: 0.001632  loss: 0.007161  eta: <1min   tot: 0h6m31s  (96.1%)80.7%  lr: 0.001592  loss: 0.007160  eta: <1min   tot: 0h6m31s  (96.1%)80.8%  lr: 0.001582  loss: 0.007154  eta: <1min   tot: 0h6m31s  (96.2%)81.0%  lr: 0.001572  loss: 0.007152  eta: <1min   tot: 0h6m32s  (96.2%)81.6%  lr: 0.001512  loss: 0.007180  eta: <1min   tot: 0h6m32s  (96.3%)82.3%  lr: 0.001462  loss: 0.007164  eta: <1min   tot: 0h6m33s  (96.5%)82.4%  lr: 0.001452  loss: 0.007160  eta: <1min   tot: 0h6m33s  (96.5%)83.3%  lr: 0.001331  loss: 0.007157  eta: <1min   tot: 0h6m34s  (96.7%)84.3%  lr: 0.001181  loss: 0.007157  eta: <1min   tot: 0h6m36s  (96.9%)84.6%  lr: 0.001111  loss: 0.007162  eta: <1min   tot: 0h6m36s  (96.9%)84.7%  lr: 0.001111  loss: 0.007165  eta: <1min   tot: 0h6m36s  (96.9%)85.2%  lr: 0.001051  loss: 0.007173  eta: <1min   tot: 0h6m37s  (97.0%)85.3%  lr: 0.001051  loss: 0.007172  eta: <1min   tot: 0h6m37s  (97.1%)88.2%  lr: 0.000771  loss: 0.007147  eta: <1min   tot: 0h6m40s  (97.6%)88.7%  lr: 0.000681  loss: 0.007159  eta: <1min   tot: 0h6m40s  (97.7%)89.2%  lr: 0.000661  loss: 0.007154  eta: <1min   tot: 0h6m40s  (97.8%)89.5%  lr: 0.000641  loss: 0.007151  eta: <1min   tot: 0h6m41s  (97.9%)89.6%  lr: 0.000621  loss: 0.007148  eta: <1min   tot: 0h6m41s  (97.9%)%  lr: 0.000621  loss: 0.007147  eta: <1min   tot: 0h6m41s  (98.0%)92.9%  lr: 0.000280  loss: 0.007132  eta: <1min   tot: 0h6m45s  (98.6%)93.7%  lr: 0.000240  loss: 0.007133  eta: <1min   tot: 0h6m45s  (98.7%)93.9%  lr: 0.000210  loss: 0.007125  eta: <1min   tot: 0h6m45s  (98.8%)95.6%  lr: 0.000120  loss: 0.007107  eta: <1min   tot: 0h6m46s  (99.1%)97.9%  lr: 0.000030  loss: 0.007123  eta: <1min   tot: 0h6m47s  (99.6%)\n",
      " ---+++                Epoch    4 Train error : 0.00689663 +++--- ☃\n",
      "Saving model to file : StarSpace_embeddings\n",
      "Saving model in tsv format : StarSpace_embeddings.tsv\n"
     ]
    }
   ],
   "source": [
    "!starspace train -trainFile data/train_prepared.tsv -model StarSpace_embeddings \\\n",
    "-trainMode 3 \\\n",
    "-adagrad true \\\n",
    "-ngrams 1 \\\n",
    "-epoch 5 \\\n",
    "-dim 100 \\\n",
    "-similarity cosine \\\n",
    "-minCount 2 \\\n",
    "-verbose true \\\n",
    "-fileFormat labelDoc \\\n",
    "-negSearchLimit 10 \\\n",
    "-lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = {}\n",
    "for line in open('StarSpace_embeddings.tsv', encoding='utf-8'):\n",
    "    word, *vec = line.strip().split('\\t')\n",
    "    starspace_embeddings[word] = np.array(vec, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.529 | Hits@   1: 0.529\n",
      "DCG@   5: 0.638 | Hits@   5: 0.736\n",
      "DCG@  10: 0.655 | Hits@  10: 0.790\n",
      "DCG@ 100: 0.686 | Hits@ 100: 0.936\n",
      "DCG@ 500: 0.693 | Hits@ 500: 0.987\n",
      "DCG@1000: 0.694 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 79\t69\t70\t17\t22\t94\t83\t61\t40\t32\t25\t47\t5\t64\t74\t56\t7\t38\t42\t16\t66\t36\t15\t86\t73\t20\t62\t93\t18\t2\t46\t75\t63\t58\t7...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.01929389126598835\n",
      "-0.02872721292078495\n",
      "0.0460561104118824\n",
      "0.0852593332529068\n",
      "0.0243055559694767\n",
      "-0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n",
      "Task StarSpaceRanks: 79\t69\t70\t17\t22\t94\t83\t61\t40\t32\t25\t47\t5\t64\t74\t56\t7\t38\t42\t16\t66\t36\t15\t86\t73\t20\t62\t93\t18\t2\t46\t75\t63\t58\t7...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = \"paragsonar35@gmail.com\"\n",
    "STUDENT_TOKEN = \"FC9QzQ00XcSwdhlR\"\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
